{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](img/330-banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Lecture 5: Preprocessing and `sklearn` pipelines\n",
    "\n",
    "UBC 2020-21\n",
    "\n",
    "Instructor: Varada Kolhatkar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import HTML\n",
    "\n",
    "sys.path.append(\"code/.\")\n",
    "\n",
    "import mglearn\n",
    "from IPython.display import display\n",
    "from plotting_functions import *\n",
    "\n",
    "# Classifiers and regressors\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "\n",
    "# Preprocessing and pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# train test split and cross validation\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import (\n",
    "    MinMaxScaler,\n",
    "    OneHotEncoder,\n",
    "    OrdinalEncoder,\n",
    "    StandardScaler,\n",
    ")\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from utils import *\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lecture plan for today\n",
    "\n",
    "- Announcements (~2 mins)\n",
    "- Commentary, exercises, Q&A on pre-watched videos (~30 mins) \n",
    "- Watch [sklearn pipelines video]() (~12 mins)\n",
    "- Q&A (~3 mins)\n",
    "- Break\n",
    "- Watch [One-hot encoding video]() (~9 mins)  \n",
    "- Exercises and Q&A (~10 mins )\n",
    "- Summary and wrap up (~5 mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Announcements\n",
    "- Homework 3 is out. (Due date: Oct 4th.) Please start early. \n",
    "- We're mostly done with Homework 1 grading. The grades will be released either today  or tomorrow morning. \n",
    "- Homework 2 solutions are posted on Canvas. Please do not share them with anyone or do not post them anywhere. \n",
    "- No homework pull requests, please. \n",
    "- Morale poll\n",
    "    - ~82% of you are doing OK but ~18% of you are feeling overwhelmed 😔.\n",
    "    - Please reach out and make use of tutorials and office hours if you are struggling.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning outcomes\n",
    "\n",
    "From this lecture, you will be able to \n",
    "\n",
    "- explain motivation for preprocessing in supervised machine learning;\n",
    "- identify when to implement feature transformations such as imputation, scaling, and one-hot encoding in a machine learning model development pipeline; \n",
    "- use `sklearn` transformers for applying feature transformations on your dataset;\n",
    "- discuss golden rule in the context of feature transformations;\n",
    "- use `sklearn.pipeline.Pipeline` and `sklearn.pipeline.make_pipeline` to build a preliminary machine learning pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation and big picture [[video](https://youtu.be/xx9HlmzORRk)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- So far we have seen\n",
    "    - Three ML models (decision trees, $k$-NNs, SVMs with RBF kernel)\n",
    "    - ML fundamentals (train-validation-test split, cross-validation, the fundamental tradeoff, the golden rule)\n",
    "- Are we ready to do machine learning on real-world datasets?\n",
    "    - Very often real-world datasets need preprocessing before we use them to build ML models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: $k$-nearest neighbours on the Spotify dataset\n",
    "\n",
    "- In lab1 you used `DecisionTreeClassifier` to predict whether the user would like a particular song or not. \n",
    "- Can we use $k$-NN classifier for this task? \n",
    "- Intuition: To predict whether the user likes a particular song or not (query point) \n",
    "   - find the songs that are closest to the query point\n",
    "   - let them vote on the target\n",
    "   - take the majority vote as the target for the query point\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "spotify_df = pd.read_csv(\"data/spotify.csv\", index_col=0)\n",
    "train_df, test_df = train_test_split(spotify_df, test_size=0.20, random_state=123)\n",
    "X_train, y_train = (\n",
    "    train_df.drop(columns=[\"song_title\", \"artist\", \"target\"]),\n",
    "    train_df[\"target\"],\n",
    ")\n",
    "X_test, y_test = (\n",
    "    test_df.drop(columns=[\"song_title\", \"artist\", \"target\"]),\n",
    "    test_df[\"target\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean validation score 0.508\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_score</th>\n",
       "      <th>train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000645</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>0.507740</td>\n",
       "      <td>0.507752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000482</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.507740</td>\n",
       "      <td>0.507752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.507740</td>\n",
       "      <td>0.507752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000450</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.506211</td>\n",
       "      <td>0.508133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000451</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.509317</td>\n",
       "      <td>0.507359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit_time  score_time  test_score  train_score\n",
       "0  0.000645    0.000255    0.507740     0.507752\n",
       "1  0.000482    0.000197    0.507740     0.507752\n",
       "2  0.000454    0.000193    0.507740     0.507752\n",
       "3  0.000450    0.000195    0.506211     0.508133\n",
       "4  0.000451    0.000191    0.509317     0.507359"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy = DummyClassifier(strategy=\"most_frequent\")\n",
    "scores = cross_validate(dummy, X_train, y_train, return_train_score=True)\n",
    "print(\"Mean validation score %0.3f\" % (np.mean(scores[\"test_score\"])))\n",
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean validation score 0.546\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_score</th>\n",
       "      <th>train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002364</td>\n",
       "      <td>0.008063</td>\n",
       "      <td>0.563467</td>\n",
       "      <td>0.717829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002073</td>\n",
       "      <td>0.007181</td>\n",
       "      <td>0.535604</td>\n",
       "      <td>0.721705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001917</td>\n",
       "      <td>0.007408</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.708527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001993</td>\n",
       "      <td>0.007992</td>\n",
       "      <td>0.537267</td>\n",
       "      <td>0.721921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001837</td>\n",
       "      <td>0.007380</td>\n",
       "      <td>0.562112</td>\n",
       "      <td>0.711077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit_time  score_time  test_score  train_score\n",
       "0  0.002364    0.008063    0.563467     0.717829\n",
       "1  0.002073    0.007181    0.535604     0.721705\n",
       "2  0.001917    0.007408    0.529412     0.708527\n",
       "3  0.001993    0.007992    0.537267     0.721921\n",
       "4  0.001837    0.007380    0.562112     0.711077"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "scores = cross_validate(knn, X_train, y_train, return_train_score=True)\n",
    "print(\"Mean validation score %0.3f\" % (np.mean(scores[\"test_score\"])))\n",
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>energy</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>key</th>\n",
       "      <th>liveness</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>valence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>0.229000</td>\n",
       "      <td>0.494</td>\n",
       "      <td>147893</td>\n",
       "      <td>0.666</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0469</td>\n",
       "      <td>-9.743</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>140.832</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>0.000289</td>\n",
       "      <td>0.771</td>\n",
       "      <td>227143</td>\n",
       "      <td>0.949</td>\n",
       "      <td>0.602000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.5950</td>\n",
       "      <td>-4.712</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1750</td>\n",
       "      <td>111.959</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     acousticness  danceability  duration_ms  energy  instrumentalness  key  \\\n",
       "842      0.229000         0.494       147893   0.666          0.000057    9   \n",
       "654      0.000289         0.771       227143   0.949          0.602000    8   \n",
       "\n",
       "     liveness  loudness  mode  speechiness    tempo  time_signature  valence  \n",
       "842    0.0469    -9.743     0       0.0351  140.832             4.0    0.704  \n",
       "654    0.5950    -4.712     1       0.1750  111.959             4.0    0.372  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_songs = X_train.sample(2, random_state=42)\n",
    "two_songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0.        , 79250.00543825],\n",
       "       [79250.00543825,     0.        ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euclidean_distances(two_songs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider only two features: `duration_ms` and `tempo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>tempo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>147893</td>\n",
       "      <td>140.832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>227143</td>\n",
       "      <td>111.959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     duration_ms    tempo\n",
       "842       147893  140.832\n",
       "654       227143  111.959"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_songs_subset = two_songs[[\"duration_ms\", \"tempo\"]]\n",
    "two_songs_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0.        , 79250.00525962],\n",
       "       [79250.00525962,     0.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euclidean_distances(two_songs_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see any problem? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The distance is completely dominated by the the features with larger values\n",
    "- The features with smaller values are being ignored. \n",
    "- Does it matter? \n",
    "    - Yes! Scale is based on how data was collected. \n",
    "    - Features on a smaller scale can be highly informative and there is no good reason to ignore them.\n",
    "    - We want our model to be robust and not sensitive to the scale. \n",
    "- Was this a problem for decision trees?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Scaling using `scikit-learn`'s [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    "\n",
    "- We'll use `scikit-learn`'s [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html), which is a `transformer`.   \n",
    "- Only focus on the syntax for now. We'll talk about scaling in a bit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>energy</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>key</th>\n",
       "      <th>liveness</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>valence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.697633</td>\n",
       "      <td>-0.194548</td>\n",
       "      <td>-0.398940</td>\n",
       "      <td>-0.318116</td>\n",
       "      <td>-0.492359</td>\n",
       "      <td>1.275623</td>\n",
       "      <td>-0.737898</td>\n",
       "      <td>0.395794</td>\n",
       "      <td>-1.280599</td>\n",
       "      <td>-0.617752</td>\n",
       "      <td>-0.293827</td>\n",
       "      <td>0.138514</td>\n",
       "      <td>-0.908149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.276291</td>\n",
       "      <td>0.295726</td>\n",
       "      <td>-0.374443</td>\n",
       "      <td>-0.795552</td>\n",
       "      <td>0.598355</td>\n",
       "      <td>-1.487342</td>\n",
       "      <td>-0.438792</td>\n",
       "      <td>-0.052394</td>\n",
       "      <td>0.780884</td>\n",
       "      <td>2.728394</td>\n",
       "      <td>-0.802595</td>\n",
       "      <td>-3.781179</td>\n",
       "      <td>-1.861238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.599540</td>\n",
       "      <td>1.110806</td>\n",
       "      <td>-0.376205</td>\n",
       "      <td>-0.946819</td>\n",
       "      <td>-0.492917</td>\n",
       "      <td>0.446734</td>\n",
       "      <td>-0.399607</td>\n",
       "      <td>-0.879457</td>\n",
       "      <td>0.780884</td>\n",
       "      <td>2.534909</td>\n",
       "      <td>0.191274</td>\n",
       "      <td>0.138514</td>\n",
       "      <td>0.575870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.307150</td>\n",
       "      <td>1.809445</td>\n",
       "      <td>-0.654016</td>\n",
       "      <td>-1.722063</td>\n",
       "      <td>-0.492168</td>\n",
       "      <td>0.170437</td>\n",
       "      <td>-0.763368</td>\n",
       "      <td>-1.460798</td>\n",
       "      <td>-1.280599</td>\n",
       "      <td>-0.608647</td>\n",
       "      <td>-0.839616</td>\n",
       "      <td>0.138514</td>\n",
       "      <td>1.825358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.634642</td>\n",
       "      <td>0.491835</td>\n",
       "      <td>-0.131344</td>\n",
       "      <td>1.057468</td>\n",
       "      <td>2.723273</td>\n",
       "      <td>0.170437</td>\n",
       "      <td>-0.458384</td>\n",
       "      <td>-0.175645</td>\n",
       "      <td>-1.280599</td>\n",
       "      <td>-0.653035</td>\n",
       "      <td>-0.074294</td>\n",
       "      <td>0.138514</td>\n",
       "      <td>-0.754491</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   acousticness  danceability  duration_ms    energy  instrumentalness  \\\n",
       "0     -0.697633     -0.194548    -0.398940 -0.318116         -0.492359   \n",
       "1     -0.276291      0.295726    -0.374443 -0.795552          0.598355   \n",
       "2     -0.599540      1.110806    -0.376205 -0.946819         -0.492917   \n",
       "3     -0.307150      1.809445    -0.654016 -1.722063         -0.492168   \n",
       "4     -0.634642      0.491835    -0.131344  1.057468          2.723273   \n",
       "\n",
       "        key  liveness  loudness      mode  speechiness     tempo  \\\n",
       "0  1.275623 -0.737898  0.395794 -1.280599    -0.617752 -0.293827   \n",
       "1 -1.487342 -0.438792 -0.052394  0.780884     2.728394 -0.802595   \n",
       "2  0.446734 -0.399607 -0.879457  0.780884     2.534909  0.191274   \n",
       "3  0.170437 -0.763368 -1.460798 -1.280599    -0.608647 -0.839616   \n",
       "4  0.170437 -0.458384 -0.175645 -1.280599    -0.653035 -0.074294   \n",
       "\n",
       "   time_signature   valence  \n",
       "0        0.138514 -0.908149  \n",
       "1       -3.781179 -1.861238  \n",
       "2        0.138514  0.575870  \n",
       "3        0.138514  1.825358  \n",
       "4        0.138514 -0.754491  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()  # create feature trasformer object\n",
    "scaler.fit(X_train)  # fitting the transformer on the train split\n",
    "X_train_scaled = scaler.transform(X_train)  # transforming the train split\n",
    "X_test_scaled = scaler.transform(X_test)  # transforming the test split\n",
    "pd.DataFrame(X_train_scaled, columns=X_train.columns).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `fit` and `transform` paradigm for transformers\n",
    "- `sklearn` uses `fit` and `transform` paradigms for feature transformations. \n",
    "- We `fit` the transformer on the train split and then transform the train split as well as the test split. \n",
    "- We apply the same transformations on the test split. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `sklearn` API summary: estimators\n",
    "\n",
    "Suppose `model` is a classification or regression model. \n",
    "\n",
    "```\n",
    "model.fit(X_train, y_train)\n",
    "X_train_predictions = model.predict(X_train)\n",
    "X_test_predictions = model.predict(X_test)\n",
    "```    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `sklearn` API summary: transformers\n",
    "\n",
    "Suppose `transformer` is a transformer used to change the input representation, for example, to tackle missing values or to scales numeric features.\n",
    "\n",
    "```\n",
    "transformer.fit(X_train, [y_train])\n",
    "X_train_transformed = transformer.transform(X_train)\n",
    "X_test_transformed = transformer.transform(X_test)\n",
    "```  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- You can pass `y_train` in `fit` but it's usually ignored. It allows you to pass it just to be consistent with usual usage of `sklearn`'s `fit` method.   \n",
    "- You can also carry out fitting and transforming in one call using `fit_transform`. But be mindful to use it only on the train split and **not** on the test split. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Do you expect `DummyClassifier` results to change after scaling the data? \n",
    "- Let's check whether scaling makes any difference for $k$-NNs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.726\n",
      "Test score: 0.552\n"
     ]
    }
   ],
   "source": [
    "knn_unscaled = KNeighborsClassifier()\n",
    "knn_unscaled.fit(X_train, y_train)\n",
    "print(\"Train score: %0.3f\" % (knn_unscaled.score(X_train, y_train)))\n",
    "print(\"Test score: %0.3f\" % (knn_unscaled.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.798\n",
      "Test score: 0.686\n"
     ]
    }
   ],
   "source": [
    "knn_scaled = KNeighborsClassifier()\n",
    "knn_scaled.fit(X_train_scaled, y_train)\n",
    "print(\"Train score: %0.3f\" % (knn_scaled.score(X_train_scaled, y_train)))\n",
    "print(\"Test score: %0.3f\" % (knn_scaled.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The scores with scaled data are better compared to the unscaled data in case of $k$-NNs.\n",
    "- I am not carrying out cross-validation here for a reason that we'll look into soon. \n",
    "- Note that I am a bit sloppy here and using the test set several times for teaching purposes. But when you build an ML pipeline, please do assessment on the test set only once.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Common preprocessing techniques\n",
    "\n",
    "Some commonly performed feature transformation include:  \n",
    "- Imputation: Tackling missing values\n",
    "- Scaling: Scaling of numeric features\n",
    "- One-hot encoding: Tackling categorical variables      \n",
    "    \n",
    "\n",
    "We can have one lecture on each of them! In this lesson our goal is to getting familiar with them so that we can use them to build ML pipelines. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the next part of this lecture, we'll build an ML pipeline using [California housing prices regression dataset](https://www.kaggle.com/harrywang/housing). In the process, we will talk about different feature transformations and how can we apply them so that we do not violate the golden rule. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break (5 min)\n",
    "\n",
    "![](img/eva-coffee.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical features [[video](https://youtu.be/2mJ9rAhMMl0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Recall that we had dropped the categorical feature `ocean_proximity` feature from the dataframe. But it could potentially be a useful feature in this task. \n",
    "\n",
    "- Let's create our `X_train` and and `X_test` again by keeping the feature in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['median_house_value'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/80/kr9rkqfj4w78h49djkz8yy9r0000gp/T/ipykernel_20538/3781118800.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"median_house_value\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"median_house_value\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"median_house_value\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/cpsc330/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/cpsc330/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4899\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[0;36m1.0\u001b[0m     \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4900\u001b[0m         \"\"\"\n\u001b[0;32m-> 4901\u001b[0;31m         return super().drop(\n\u001b[0m\u001b[1;32m   4902\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4903\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/cpsc330/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4149\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4150\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4152\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/cpsc330/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   4183\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4184\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4185\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4186\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/cpsc330/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6016\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6017\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6018\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6019\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6020\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['median_house_value'] not found in axis\""
     ]
    }
   ],
   "source": [
    "X_train = train_df.drop(columns=[\"median_house_value\"])\n",
    "y_train = train_df[\"median_house_value\"]\n",
    "\n",
    "X_test = test_df.drop(columns=[\"median_house_value\"])\n",
    "y_test = test_df[\"median_house_value\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's try to build a `KNeighborRegressor` on this data using our pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This failed because we have non-numeric data. \n",
    "- Imagine how $k$-NN would calculate distances when you have non-numeric features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we use this feature in the model? \n",
    "- In `scikit-learn`, most algorithms require numeric inputs.\n",
    "- Decision trees could theoretically work with categorical features.  \n",
    "    - However, the sklearn implementation does not support this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the options? \n",
    "\n",
    "- Drop the column (not recommended)\n",
    "    - If you know that the column is not relevant to the target in any way you may drop it. \n",
    "- We can transform categorical features to numeric ones so that we can use them in the model.     \n",
    "    - [Ordinal encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html) (occasionally recommended)\n",
    "    - One-hot encoding (recommended in most cases) (this lecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_toy = pd.DataFrame(\n",
    "    {\n",
    "        \"language\": [\n",
    "            \"English\",\n",
    "            \"Vietnamese\",\n",
    "            \"English\",\n",
    "            \"Mandarin\",\n",
    "            \"English\",\n",
    "            \"English\",\n",
    "            \"Mandarin\",\n",
    "            \"English\",\n",
    "            \"Vietnamese\",\n",
    "            \"Mandarin\",\n",
    "            \"French\",\n",
    "            \"Spanish\",\n",
    "            \"Mandarin\",\n",
    "            \"Hindi\",\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "X_toy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinal encoding (occasionally recommended)\n",
    "\n",
    "- Here we simply assign an integer to each of our unique categorical labels. \n",
    "- We can use sklearn's [`OrdinalEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "enc = OrdinalEncoder()\n",
    "enc.fit(X_toy)\n",
    "X_toy_ord = enc.transform(X_toy)\n",
    "df = pd.DataFrame(\n",
    "    data=X_toy_ord,\n",
    "    columns=[\"language_enc\"],\n",
    "    index=X_toy.index,\n",
    ")\n",
    "pd.concat([X_toy, df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the problem with this approach? \n",
    "- We have imposed ordinality on the categorical data.\n",
    "- For example, imagine when you are calculating distances. Is it fair to say that French and Hindi are closer than French and Spanish? \n",
    "- In general, label encoding is useful if there is ordinality in your data and capturing it is important for your problem, e.g., `[cold, warm, hot]`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot encoding (OHE)\n",
    "- Create new binary columns to represent our categories.\n",
    "- If we have $c$ categories in our column.\n",
    "    - We create $c$ new binary columns to represent those categories.\n",
    "- Example: Imagine a language column which has the information on whether you \n",
    "\n",
    "- We can use sklearn's [`OneHotEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "One-hot encoding is called one-hot because only one of the newly created features is 1 for each data point. \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "enc.fit(X_toy)\n",
    "X_toy_ohe = enc.transform(X_toy)\n",
    "pd.DataFrame(\n",
    "    data=X_toy_ohe,\n",
    "    columns=enc.get_feature_names([\"language\"]),\n",
    "    index=X_toy.index,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's do it on our housing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse=False, dtype=\"int\")\n",
    "ohe.fit(X_train[[\"ocean_proximity\"]])\n",
    "X_imp_ohe_train = ohe.transform(X_train[[\"ocean_proximity\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can look at the new features created using `categories_` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_ohe = pd.DataFrame(\n",
    "    data=X_imp_ohe_train,\n",
    "    columns=ohe.get_feature_names([\"ocean_proximity\"]),\n",
    "    index=X_train.index,\n",
    ")\n",
    "transformed_ohe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{seealso} \n",
    "One-hot encoded variables are also referred to as **dummy variables**. \n",
    "You will often see people using [`get_dummies` method of pandas](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html) to convert categorical variables into dummy variables. That said, using `sklearn`'s `OneHotEncoder` has the advantage of making it easy to treat training and test set in a consistent way.  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ❓❓ Questions for class discussion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### True/False: Pipelines and one-hot encoding\n",
    "\n",
    "1. You can \"glue\" together imputation and scaling of numeric features and `scikit-learn` classifier object within a single pipeline.  \n",
    "2. You can \"glue\" together scaling of numeric features, one-hot encoding of categorical features, and `scikit-learn` classifier object within a single pipeline.  \n",
    "3. Once you have a `scikit-learn` pipeline object you can call `fit`, `predict`, and `score` on it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More True/False on pipelines and one-hot encoding\n",
    "\n",
    "4. You can carry out data splitting within `scikit-learn` pipeline. \n",
    "5. We have to be careful of the order we put each transformation and model in a pipeline.\n",
    "6. Pipelines will `fit` and `transform` on the training fold and only `transform` on the validation fold during cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What did we learn today? \n",
    "\n",
    "- Motivation for preprocessing\n",
    "- Common preprocessing steps\n",
    "    - Imputation \n",
    "    - Scaling\n",
    "    - One-hot encoding\n",
    "- Golden rule in the context of preprocessing\n",
    "- Building simple supervised machine learning pipelines using `sklearn.pipeline.make_pipeline`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem: Different transformations on different columns\n",
    "- How do we put this together with other columns in the data before fitting the regressor? \n",
    "- Before we fit our regressor, we want to apply different transformations on different columns \n",
    "    - Numeric columns\n",
    "        - imputation \n",
    "        - scaling         \n",
    "    - Categorical columns \n",
    "        - imputation \n",
    "        - one-hot encoding        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coming up: sklearn's [`ColumnTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html)!!** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:cpsc330]",
   "language": "python",
   "name": "conda-env-cpsc330-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
